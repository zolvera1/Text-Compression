#WHAT THIS IS 
A decoder and corresponding encoder in Java capable of compressing arbitrary input data using Huffman codes. To test your decoder, an already-compressed binary file is provided. Your decoder algorithm should be able to correctly decompress the given binary file to produce an uncompressed text file containing English text. Your encoder should be able to take any raw text file and produce a compressed version that is decodable by your decoder. More detailed specifications for the compressed format, encoder algorithm, and decoder algorithm are given in the sections below.
One obstacle faced by programmers of data compression algorithms is that operating systems and CPUs typically restrict I/O operations to only allow manipulation of files in byte-sized increments. Variable length codes, including Huffman codes, are often not exactly 8 bits long and therefore often do not line up on a byte boundary. To dealw ith this, the code has prewritten classes that wrap Java’s built-in InputStream and OutputStream interfaces, enabling bit-sized read and write functionality by buffering bits in memory until enough bits are accumulated to perform a byte operation on the underlying stream. Use the read() method provided by the InputStreamBitSource class to read individual bits from an InputStream, and use the write() method from the OutputStreamBitSink class to write bits to a OutputStream.

The data within a compressed file represents a sequence of Huffman-coded bytes. Each of the 256 unique 8-bit patterns (i.e. 00000000, 00000001, 00000010, ..., 11111111) that a byte may represent is assigned its own prefix-free Huffman code. Thus, there are 256 different Huffman codewords referring to the 256 unique byte symbols.
In the compressed file, bytes are represented by their corresponding Huffman codeword. The first step to decoding the file is therefore to construct a table which maps the Huffman codewords to the bytes they represent. This mapping is signaled at the very beginning of the compressed file using the canonical Huffman tree method described in class (see book for details). The 256 lengths of the 256 Huffman codewords are therefore signaled at the beginning of the compressed file (this is described
 
more precisely below). Knowing these code lengths, one can construct the canonical Huffman tree using the deterministic algorithm from class. The canonical tree then provides the mapping between the codewords used in the compressed file and their corresponding byte symbols.


Part 2 – Encoding Huffman Codes
An encoder was written  capable of compressing raw input files and producing encoded files that can be decoded by the decoder from Part 1. Thus, your encoder should output a file with the same structure as in Part 1; the first 256 bytes written should represent the codeword lengths in canonical tree form, the next 4 bytes should represent the number of encoded symbols, and the rest of the bytes should be filled with codewords.
The encoding process is essentially the same as the decoding process, only in reverse – instead of reading bytes, you are writing them. Instead of transforming Huffman codewords into bytes, you will be transforming bytes into Huffman codewords.
When you sit down to start writing your encoder, you’ll notice that the first thing you have to output to the encoded file is a list of 256 codeword lengths. While one possibility is to simply reuse the same codeword lengths (and therefore the same codewords) from the encoded file given with this assignment, you should not do this. Instead, your encoder should generate optimal Huffman codeword lengths for the specific content that is being encoded, according to the symbol frequencies present in the input data. To do this, you will need to scan the entire source file and track how many of each symbol is encountered. These values can be used to calculate the average probabilities of each symbol

across the entire input file. Next, use this information to construct an optimal, minimum variance Huffman tree according to the algorithm described in class (and also explained in the book). This tree will provide the correct lengths for each codeword according to their probabilities in the source input. Finally, throw away the generated Huffman tree and use the codeword lengths alone to re-generate a new Huffman tree that represents the canonical tree for those lengths. You should be able to re-use some code from Part 1 to do this.


